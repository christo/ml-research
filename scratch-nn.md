# Scratch Neural Network

What maths operations are required to build a neural network with back propagation? 

* activation functions: 
    * ReLU - Rectified Linear Unit
    * tanh - sigmoidal maps to -1->1
    * sigmoid - logistic function maps to 0-1
* forward propagation
    * activation function
* back propagation
    * derivatives of each layer step
* matrix dot product
* softmax function
    * used as last activation function to normalise 
    * generalisation of logistic function to multiple dimensions
* hyperparameters
    * learning rate (alpha)


softmax

Scratch maths primitives:

* binary arithmetic operations
    * addition, multiplication, subtraction, division
    * modulus
* unary functions 
    * random(x,y) bounded range, integer or double
    * round(x) (half up)
    * abs(x)
    * floor(x)
    * ceiling(x)
    * sqrt(x)
    * sin(x)
    * cos(x)
    * tan(x)
    * asin(x)
    * acos(x)
    * atan(x)
    * ln(x)
    * log(x)
    * e ^ x
    * 10 ^ x
* relational operations
    * less than, greater than, equality
* logical operations
    * and, or, not

